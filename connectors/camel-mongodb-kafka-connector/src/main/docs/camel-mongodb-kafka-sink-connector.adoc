// kafka-connector options: START
[[camel-mongodb-kafka-connector-sink]]
= camel-mongodb-kafka-connector sink configuration

When using camel-mongodb-kafka-connector as sink make sure to use the following Maven dependency to have support for the connector:

[source,xml]
----
<dependency>
  <groupId>org.apache.camel.kafkaconnector</groupId>
  <artifactId>camel-mongodb-kafka-connector</artifactId>
  <version>x.x.x</version>
  <!-- use the same version as your Camel Kafka connector version -->
</dependency>
----

To use this Sink connector in Kafka connect you'll need to set the following connector.class

[source,java]
----
connector.class=org.apache.camel.kafkaconnector.mongodb.CamelMongodbSinkConnector
----


The camel-mongodb sink connector supports 26 options, which are listed below.



[width="100%",cols="2,5,^1,2",options="header"]
|===
| Name | Description | Default | Priority
| *camel.sink.path.connectionBean* | Sets the connection bean reference used to lookup a client for connecting to a database. | null | HIGH
| *camel.sink.endpoint.collection* | Sets the name of the MongoDB collection to bind to this endpoint | null | MEDIUM
| *camel.sink.endpoint.collectionIndex* | Sets the collection index (JSON FORMAT : { field1 : order1, field2 : order2}) | null | MEDIUM
| *camel.sink.endpoint.createCollection* | Create collection during initialisation if it doesn't exist. Default is true. | true | MEDIUM
| *camel.sink.endpoint.database* | Sets the name of the MongoDB database to target | null | MEDIUM
| *camel.sink.endpoint.mongoConnection* | Sets the connection bean used as a client for connecting to a database. | null | MEDIUM
| *camel.sink.endpoint.operation* | Sets the operation this endpoint will execute against MongoDB. One of: [findById] [findOneByQuery] [findAll] [findDistinct] [insert] [save] [update] [remove] [bulkWrite] [aggregate] [getDbStats] [getColStats] [count] [command] | null | MEDIUM
| *camel.sink.endpoint.outputType* | Convert the output of the producer to the selected type : DocumentList Document or MongoIterable. DocumentList or MongoIterable applies to findAll and aggregate. Document applies to all other operations. One of: [DocumentList] [Document] [MongoIterable] | null | MEDIUM
| *camel.sink.endpoint.lazyStartProducer* | Whether the producer should be started lazy (on the first message). By starting lazy you can use this to allow CamelContext and routes to startup in situations where a producer may otherwise fail during starting and cause the route to fail being started. By deferring this startup to be lazy then the startup failure can be handled during routing messages via Camel's routing error handlers. Beware that when the first message is processed then creating and starting the producer may take a little time and prolong the total processing time of the processing. | false | MEDIUM
| *camel.sink.endpoint.basicPropertyBinding* | Whether the endpoint should use basic property binding (Camel 2.x) or the newer property binding with additional capabilities | false | MEDIUM
| *camel.sink.endpoint.cursorRegenerationDelay* | MongoDB tailable cursors will block until new data arrives. If no new data is inserted, after some time the cursor will be automatically freed and closed by the MongoDB server. The client is expected to regenerate the cursor if needed. This value specifies the time to wait before attempting to fetch a new cursor, and if the attempt fails, how long before the next attempt is made. Default value is 1000ms. | 1000L | MEDIUM
| *camel.sink.endpoint.dynamicity* | Sets whether this endpoint will attempt to dynamically resolve the target database and collection from the incoming Exchange properties. Can be used to override at runtime the database and collection specified on the otherwise static endpoint URI. It is disabled by default to boost performance. Enabling it will take a minimal performance hit. | false | MEDIUM
| *camel.sink.endpoint.readPreference* | Configure how MongoDB clients route read operations to the members of a replica set. Possible values are PRIMARY, PRIMARY_PREFERRED, SECONDARY, SECONDARY_PREFERRED or NEAREST One of: [PRIMARY] [PRIMARY_PREFERRED] [SECONDARY] [SECONDARY_PREFERRED] [NEAREST] | "PRIMARY" | MEDIUM
| *camel.sink.endpoint.synchronous* | Sets whether synchronous processing should be strictly used, or Camel is allowed to use asynchronous processing (if supported). | false | MEDIUM
| *camel.sink.endpoint.writeConcern* | Configure the connection bean with the level of acknowledgment requested from MongoDB for write operations to a standalone mongod, replicaset or cluster. Possible values are ACKNOWLEDGED, W1, W2, W3, UNACKNOWLEDGED, JOURNALED or MAJORITY. One of: [ACKNOWLEDGED] [W1] [W2] [W3] [UNACKNOWLEDGED] [JOURNALED] [MAJORITY] | "ACKNOWLEDGED" | MEDIUM
| *camel.sink.endpoint.writeResultAsHeader* | In write operations, it determines whether instead of returning WriteResult as the body of the OUT message, we transfer the IN message to the OUT and attach the WriteResult as a header. | false | MEDIUM
| *camel.sink.endpoint.streamFilter* | Filter condition for change streams consumer. | null | MEDIUM
| *camel.sink.endpoint.persistentId* | One tail tracking collection can host many trackers for several tailable consumers. To keep them separate, each tracker should have its own unique persistentId. | null | MEDIUM
| *camel.sink.endpoint.persistentTailTracking* | Enable persistent tail tracking, which is a mechanism to keep track of the last consumed message across system restarts. The next time the system is up, the endpoint will recover the cursor from the point where it last stopped slurping records. | false | MEDIUM
| *camel.sink.endpoint.tailTrackCollection* | Collection where tail tracking information will be persisted. If not specified, MongoDbTailTrackingConfig#DEFAULT_COLLECTION will be used by default. | null | MEDIUM
| *camel.sink.endpoint.tailTrackDb* | Indicates what database the tail tracking mechanism will persist to. If not specified, the current database will be picked by default. Dynamicity will not be taken into account even if enabled, i.e. the tail tracking database will not vary past endpoint initialisation. | null | MEDIUM
| *camel.sink.endpoint.tailTrackField* | Field where the last tracked value will be placed. If not specified, MongoDbTailTrackingConfig#DEFAULT_FIELD will be used by default. | null | MEDIUM
| *camel.sink.endpoint.tailTrackIncreasingField* | Correlation field in the incoming record which is of increasing nature and will be used to position the tailing cursor every time it is generated. The cursor will be (re)created with a query of type: tailTrackIncreasingField greater than lastValue (possibly recovered from persistent tail tracking). Can be of type Integer, Date, String, etc. NOTE: No support for dot notation at the current time, so the field should be at the top level of the document. | null | MEDIUM
| *camel.component.mongodb.mongoConnection* | Shared client used for connection. All endpoints generated from the component will share this connection client. | null | MEDIUM
| *camel.component.mongodb.lazyStartProducer* | Whether the producer should be started lazy (on the first message). By starting lazy you can use this to allow CamelContext and routes to startup in situations where a producer may otherwise fail during starting and cause the route to fail being started. By deferring this startup to be lazy then the startup failure can be handled during routing messages via Camel's routing error handlers. Beware that when the first message is processed then creating and starting the producer may take a little time and prolong the total processing time of the processing. | false | MEDIUM
| *camel.component.mongodb.basicPropertyBinding* | Whether the component should use basic property binding (Camel 2.x) or the newer property binding with additional capabilities | false | LOW
|===



The camel-mongodb sink connector has no converters out of the box.





The camel-mongodb sink connector has no transforms out of the box.





The camel-mongodb sink connector has no aggregation strategies out of the box.
// kafka-connector options: END

[NOTE]
====
This connector requires the usage of Json Converter for his value converter so you'll have to set the following:

value.converter=org.apache.kafka.connect.json.JsonConverter
====
